class: middle, center, title-slide

# –ú–µ—Ç–æ–¥–∏ —á–∏—Å–µ–ª—å–Ω–æ—ó –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó

–õ–µ–∫—Ü—ñ—è 6: –ê–¥–∞–ø—Ç–∏–≤–Ω—ñ –º–µ—Ç–æ–¥–∏ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
<br><br>
–ö–æ—á—É—Ä–∞ –Æ—Ä—ñ–π –ü–µ—Ç—Ä–æ–≤–∏—á<br>
[iuriy.kochura@gmail.com](mailto:iuriy.kochura@gmail.com) <br>
<a href="https://t.me/y_kochura">@y_kochura</a> <br>

---

class:  black-slide, 
background-image: url(./figures/lec1/blog-header-cost-optimization-examples.jpg)
background-size: cover

# –°—å–æ–≥–æ–¥–Ω—ñ

.larger-x[ <p class="shadow" style="line-height: 200%;">–Ø–∫ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏? <br>
  –ê–¥–∞–ø—Ç–∏–≤–Ω—ñ –º–µ—Ç–æ–¥–∏: <br>

üéôÔ∏è AdaGrad <br>
üéôÔ∏è RMSProp  <br>
üéôÔ∏è Adam <br> 
</p>]

---


class: blue-slide, middle, center
count: false

.larger-xx[–ê–¥–∞–ø—Ç–∏–≤–Ω—ñ –º–µ—Ç–æ–¥–∏]

---

class: middle

# –ê–¥–∞–ø—Ç–∏–≤–Ω—ñ –º–µ—Ç–æ–¥–∏  

–í–µ–ª–∏—á–∏–Ω–∞ –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ —á–∞—Å—Ç–æ —Å–∏–ª—å–Ω–æ –≤—ñ–¥—Ä—ñ–∑–Ω—è—î—Ç—å—Å—è –º—ñ–∂ —à–∞—Ä–∞–º–∏ –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ, —Ç–æ–º—É –≥–ª–æ–±–∞–ª—å–Ω–∞ —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –º–æ–∂–µ –Ω–µ –ø—Ä–∞—Ü—é–≤–∞—Ç–∏ –Ω–∞–ª–µ–∂–Ω–∏–º —á–∏–Ω–æ–º.

*–ó–∞–≥–∞–ª—å–Ω–∞ —ñ–¥–µ—è.* –ó–∞–º—ñ—Å—Ç—å —Ç–æ–≥–æ, —â–æ–± –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –æ–¥–Ω–∞–∫–æ–≤—É —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –∫–æ–∂–Ω–æ—ó –≤–∞–≥–∏ –≤ –Ω–∞—à—ñ–π –º–µ—Ä–µ–∂—ñ, **–ø—ñ–¥—Ç—Ä–∏–º—É–π—Ç–µ –æ—Ü—ñ–Ω–∫—É –∫—Ä–∞—â–æ—ó —à–≤–∏–¥–∫–æ—Å—Ç—ñ –æ–∫—Ä–µ–º–æ –¥–ª—è –∫–æ–∂–Ω–æ—ó –≤–∞–≥–∏**.

–¢–æ—á–Ω–∏–π —Å–ø–æ—Å—ñ–± –∞–¥–∞–ø—Ç–∞—Ü—ñ—ó –¥–æ —à–≤–∏–¥–∫–æ—Å—Ç—ñ –Ω–∞–≤—á–∞–Ω–Ω—è –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ –∞–ª–≥–æ—Ä–∏—Ç–º—É, –∞–ª–µ –±—ñ–ª—å—à—ñ—Å—Ç—å –º–µ—Ç–æ–¥—ñ–≤ –∞–±–æ **–ø—Ä–∏—Å—Ç–æ—Å–æ–≤—É—é—Ç—å—Å—è** –¥–æ **–¥–∏—Å–ø–µ—Ä—Å—ñ—ó –≤–∞–≥**, –∞–±–æ –¥–æ **–ª–æ–∫–∞–ª—å–Ω–æ—ó –∫—Ä–∏–≤–∏–∑–Ω–∏** –ø—Ä–æ–±–ª–µ–º–∏.

.footnote[Credits: Aaron Defazio, Facebook AI Research]

---


class: middle

## AdaGrad

–ó–º–µ–Ω—à–µ–Ω–Ω—è –º–∞—Å—à—Ç–∞–±—É –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ –Ω–∞ –∫–≤–∞–¥—Ä–∞—Ç–Ω–∏–π –∫–æ—Ä—ñ–Ω—å —ñ–∑ —Å—É–º–∏ –∫–≤–∞–¥—Ä–∞—Ç—ñ–≤ —É—Å—ñ—Ö –π–æ–≥–æ —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å.

$$\begin{aligned}
r\_t  &=  r\_{t-1} + g\_t \odot g\_t \\\\
W\_{t+1} &= W\_t - \frac{\alpha}{\varepsilon + \sqrt{r\_t}} \odot g\_t
\end{aligned}$$

- AdaGrad –ø–æ–∑–±–∞–≤–ª—è—î –≤—ñ–¥ –Ω–µ–æ–±—Ö—ñ–¥–Ω–æ—Å—Ç—ñ –≤—Ä—É—á–Ω—É –Ω–∞–ª–∞—à—Ç–æ–≤—É–≤–∞—Ç–∏ —à–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è.
  –ë—ñ–ª—å—à—ñ—Å—Ç—å —Ä–µ–∞–ª—ñ–∑–∞—Ü—ñ–π –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å $\alpha=0.01$ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º.
- –î–æ–±—Ä–µ, –∫–æ–ª–∏ —Ü—ñ–ª—å–æ–≤–∞ —Ñ—É–Ω–∫—Ü—ñ—è –æ–ø—É–∫–ª–∞.
- $r_t$ –Ω–µ–æ–±–º–µ–∂–µ–Ω–æ –∑—Ä–æ—Å—Ç–∞—î –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è, —â–æ –º–æ–∂–µ —Å–ø—Ä–∏—á–∏–Ω–∏—Ç–∏ –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –∫—Ä–æ–∫—É —Ç–∞ –∑—Ä–µ—à—Ç–æ—é —Å—Ç–∞—Ç–∏ –Ω–µ—Å–∫—ñ–Ω—á–µ–Ω–Ω–æ –º–∞–ª–∏–º.
- $\varepsilon$ –∞–¥–∏—Ç–∏–≤–Ω–∞ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–∞, —è–∫–∞ –≥–∞—Ä–∞–Ω—Ç—É—î, —â–æ –º–∏ –Ω–µ –¥—ñ–ª–∏–º–æ –Ω–∞ 0.


???
The variable $r\_t$ accumulates past gradient variance.

---


class: middle

## RMSProp

–¢–µ —Å–∞–º–µ, —â–æ AdaGrad, –∞–ª–µ –Ω–∞–∫–æ–ø–∏—á—É—î –µ–∫—Å–ø–æ–Ω–µ–Ω—Ü—ñ–∞–ª—å–Ω–æ —Å–ø–∞–¥–Ω–µ —Å–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞.

**–ö–ª—é—á–æ–≤–∞ —ñ–¥–µ—è.** –ù–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ –∑–∞ —Å–µ—Ä–µ–¥–Ω—å–æ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∏–º –∑–Ω–∞—á–µ–Ω–Ω—è–º –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞

$$\begin{aligned}
r\_t  &=  \rho r\_{t-1} + (1-\rho) g\_t \odot g\_t \\\\
W\_{t+1} &= W\_t - \frac{\alpha}{\varepsilon + \sqrt{r\_t}} \odot g\_t
\end{aligned}$$

- –ï—Ñ–µ–∫—Ç–∏–≤–Ω—ñ—à–∏–π, –∫–æ–ª–∏ —Ü—ñ–ª—å–æ–≤–∞ —Ñ—É–Ω–∫—Ü—ñ—è –Ω–µ —î –æ–ø—É–∫–ª–æ—é.

???
An alternative is to use a leaky average in the same way we used in the momentum method, for some parameter $\rho>0$.

---


class: middle

## Adam: RMSprop –∑ —ñ–º–ø—É–ª—å—Å–æ–º

.smaller-xx[‚ÄúAdaptive Moment Estimation‚Äù]

–ü–æ–¥—ñ–±–Ω–æ –¥–æ RMSProp –∑ —ñ–º–ø—É–ª—å—Å–æ–º, –∞–ª–µ –∑ —É–º–æ–≤–∞–º–∏ –∫–æ—Ä–µ–∫—Ü—ñ—ó –∑—Å—É–≤—É –¥–ª—è –ø–µ—Ä—à–æ–≥–æ —Ç–∞ –¥—Ä—É–≥–æ–≥–æ –º–æ–º–µ–Ω—Ç—ñ–≤.

$$\begin{aligned}
p\_t  &=  \rho\_1 p\_{t-1} + (1-\rho\_1) g\_t \\\\
\hat{p}\_t &= \frac{p\_t}{1-\rho\_1^t} \\\\
r\_t  &=  \rho\_2 r\_{t-1} + (1-\rho\_2) g\_t \odot g\_t \\\\
\hat{r}\_t &= \frac{r\_t}{1-\rho\_2^t} \\\\
W\_{t+1} &= W\_t - \alpha \frac{\hat{p}\_t}{\varepsilon+\sqrt{\hat{r}\_t}}
\end{aligned}$$

- –•–æ—Ä–æ—à—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º $\rho\_1=0.9$ —ñ $\rho\_2=0.999$.
- –ü–æ–¥—ñ–±–Ω–æ –¥–æ —Ç–æ–≥–æ, —è–∫ —ñ–º–ø—É–ª—å—Å –ø–æ–∫—Ä–∞—â—É—î SGD, –≤—ñ–Ω —Ç–∞–∫–æ–∂ –ø–æ–∫—Ä–∞—â—É—î RMSProp.

???
Adam is one of the **default optimizers** in deep learning, along with SGD with momentum.

---

# –ü—Ä–∞–∫—Ç–∏—á–Ω–∞ —Å—Ç–æ—Ä–æ–Ω–∞

–î–ª—è –ø–æ–≥–∞–Ω–æ –æ–±—É–º–æ–≤–ª–µ–Ω–∏—Ö –∑–∞–¥–∞—á Adam —á–∞—Å—Ç–æ –Ω–∞–±–∞–≥–∞—Ç–æ –∫—Ä–∞—â–∏–π, –Ω—ñ–∂ SGD.

–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ Adam –∑–∞–º—ñ—Å—Ç—å RMSprop –∑–∞–≤–¥—è–∫–∏ –æ—á–µ–≤–∏–¥–Ω–∏–º –ø–µ—Ä–µ–≤–∞–≥–∞–º —ñ–º–ø—É–ª—å—Å—É.

## –ê–ª–µ, Adam –ø–æ–≥–∞–Ω–æ –≤–∏–≤—á–µ–Ω–∏–π —Ç–µ–æ—Ä–µ—Ç–∏—á–Ω–æ —Ç–∞ –º–∞—î –≤—ñ–¥–æ–º—ñ –Ω–µ–¥–æ–ª—ñ–∫–∏:

- –ó–æ–≤—Å—ñ–º –Ω–µ —Å—Ö–æ–¥–∏—Ç—å—Å—è –Ω–∞ –¥–µ—è–∫–∏—Ö –ø—Ä–æ—Å—Ç–∏—Ö –ø—Ä–∏–∫–ª–∞–¥–∞—Ö –∑–∞–¥–∞—á!
- –î–∞—î –≥—ñ—Ä—à—É –ø–æ–º–∏–ª–∫—É —É–∑–∞–≥–∞–ª—å–Ω–µ–Ω–Ω—è –¥–ª—è –±–∞–≥–∞—Ç—å–æ—Ö –∑–∞–¥–∞—á –∫–æ–º–ø‚Äô—é—Ç–µ—Ä–Ω–æ–≥–æ –∑–æ—Ä—É (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, ImageNet)
- –ü–æ—Ç—Ä—ñ–±–Ω–æ –±—ñ–ª—å—à–µ –ø–∞–º'—è—Ç—ñ, –Ω—ñ–∂ SGD
- –ú–∞—î 2 –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–º–µ–Ω—Ç—ñ–≤, —Ç–æ–º—É –º–æ–∂–µ –∑–Ω–∞–¥–æ–±–∏—Ç–∏—Å—è –¥–æ–¥–∞—Ç–∫–æ–≤–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è

.footnote[Credits: Aaron Defazio, Facebook AI Research]

---

class: middle, center

.larger-xx[[–î–µ–º–æ](https://www.deeplearning.ai/ai-notes/optimization/)]

---

class: middle, center

.larger-xx[[demo - losslandscape](https://losslandscape.com/explorer)]

---

class: end-slide, center
count: false

.larger-xxxx[üèÅ]


