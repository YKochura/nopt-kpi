class: middle, center, title-slide

# –ú–µ—Ç–æ–¥–∏ —á–∏—Å–µ–ª—å–Ω–æ—ó –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó

–õ–µ–∫—Ü—ñ—è 5: –ì—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ —Ç–∞ —ñ–º–ø—É–ª—å—Å
<br><br>
–ö–æ—á—É—Ä–∞ –Æ—Ä—ñ–π –ü–µ—Ç—Ä–æ–≤–∏—á<br>
[iuriy.kochura@gmail.com](mailto:iuriy.kochura@gmail.com) <br>
<a href="https://t.me/y_kochura">@y_kochura</a> <br>

---

class:  black-slide, 
background-image: url(./figures/lec1/blog-header-cost-optimization-examples.jpg)
background-size: cover

# –°—å–æ–≥–æ–¥–Ω—ñ

.larger-x[ <p class="shadow" style="line-height: 200%;">–Ø–∫ –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –æ–ø—Ç–∏–º—ñ–∑—É–≤–∞—Ç–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏? <br>

üéôÔ∏è –ü–∞–∫–µ—Ç–Ω–∏–π –≥—Ä–∞–¥i—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ <br>
üéôÔ∏è –°—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∏–π –≥—Ä–∞–¥i—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ <br>
üéôÔ∏è –úi–Ωi-–ø–∞–∫–µ—Ç–Ω–∏–π –≥—Ä–∞–¥i—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ <br>
üéôÔ∏è I–º–ø—É–ª—å—Å <br>  
</p>]

---


class: blue-slide, middle, center
count: false

.larger-xx[–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤]

---

class: middle

# –ú–æ–¥–µ–ª—å

–•–æ—á–∞ —Ç–µ, —â–æ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤—Å–µ—Ä–µ–¥–∏–Ω—ñ –≥–ª–∏–±–æ–∫–æ—ó –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ, –º–æ–∂–µ –±—É—Ç–∏ —Å–∫–ª–∞–¥–Ω–∏–º, —É —Å–≤–æ—ó–π –æ—Å–Ω–æ–≤—ñ —Ü–µ –ø—Ä–æ—Å—Ç–æ —Ñ—É–Ω–∫—Ü—ñ—ó. –í–æ–Ω–∏ –ø—Ä–∏–π–º–∞—é—Ç—å –¥–µ—è–∫—ñ –≤—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ —Ç–∞ –≥–µ–Ω–µ—Ä—É—é—Ç—å –¥–µ—è–∫—ñ –≤–∏—Ö—ñ–¥–Ω—ñ –¥–∞–Ω—ñ (–ø—Ä–æ–≥–Ω–æ–∑–∏).

.center.width-30[![](figures/lec5/func.png)]

.footnote[Credits: NVIDIA]

---

class: middle

# –ö–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –º–æ–¥–µ–ª—ñ

.center.width-100[![](figures/lec5/modelComp.png)]

.footnote[Credits: NVIDIA]

???
–ù–∞–≤—á–µ–Ω–∞ –º–µ—Ä–µ–∂–∞ —Å–∫–ª–∞–¥–∞—î—Ç—å—Å—è –∑ –¥–≤–æ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤:
- –û–ø–∏—Å –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∏ –Ω–µ–Ω–∞–≤—á–µ–Ω–æ—ó –º–µ—Ä–µ–∂—ñ.
- –í–∞–≥–∏, —è–∫—ñ –±—É–ª–∏ "–≤–∏–≤—á–µ–Ω—ñ" –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è –º–µ—Ä–µ–∂—ñ.

---

class: middle

# –ó–∞–≥–∞–ª—å–Ω–∏–π –ø—Ä–æ—Ü–µ—Å –Ω–∞–≤—á–∞–Ω–Ω—è –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂

1. –í–∏–∑–Ω–∞—á—Ç–µ –∑–∞–≤–¥–∞–Ω–Ω—è + –∑–±–µ—Ä—ñ—Ç—å –¥–∞–Ω—ñ
2. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑—É–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
3. –û–±–µ—Ä—ñ—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
4. –ü–æ–≤—Ç–æ—Ä—ñ—Ç—å —Ü—ñ –∫—Ä–æ–∫–∏:
.smaller-xx[
    4.1. –ü—Ä—è–º–µ –ø–æ—à–µ—Ä–µ–Ω–Ω—è –≤—Ö—ñ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö

    4.2 –û–±—á–∏—Å–ª–µ–Ω–Ω—è —Ü—ñ–ª—å–æ–≤–æ—ó —Ñ—É–Ω–∫—Ü—ñ—é –≤–∏—Ç—Ä–∞—Ç]
.smaller-xx[
    4.3 –ó–≤–æ—Ä–æ—Ç–Ω–µ –ø–æ—à–∏—Ä–µ–Ω–Ω—è: –æ–±—á–∏—Å–ª—ñ—Ç—å –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ —Ü—ñ–ª—å–æ–≤–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó –≤—Ç—Ä–∞—Ç –≤—ñ–¥–Ω–æ—Å–Ω–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤

    4.4 –û–Ω–æ–≤—ñ—Ç—å –∫–æ–∂–µ–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é –≥—Ä–∞–¥—ñ—î–Ω—Ç—ñ–≤ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ –∞–ª–≥–æ—Ä–∏—Ç–º—É –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó]

---

class:  black-slide, 
background-image: url(./figures/lec5/optimization.png)
background-size: cover

---

class: blue-slide, middle, center
count: false

.larger-xx[–û–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ–π–Ω—ñ –∞–ª–≥–æ—Ä–∏—Ç–º–∏]

---


class: middle

# –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ & –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

- –†–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó —î –Ω–∞–±—ñ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
- –ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏ vs. –ü–∞—Ä–∞–º–µ—Ç—Ä–∏

.grid[
.kol-1-2[
**–ì—ñ–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∏**
.smaller-xx[
- –ö–æ–µ—Ñ—ñ—Ü—ñ—î–Ω—Ç –ø–æ–¥—ñ–ª—É –¥–∞—Ç–∞—Å–µ—Ç—É –Ω–∞–≤—á–∞–Ω–Ω—è/–≤–∞–ª—ñ–¥–∞—Ü—ñ—è/—Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
- –®–≤–∏–¥–∫—ñ—Å—Ç—å –Ω–∞–≤—á–∞–Ω–Ω—è –≤ –∞–ª–≥–æ—Ä–∏—Ç–º–∞—Ö –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó
- –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏—Ö —à–∞—Ä—ñ–≤ —É –ù–ú
- –ö—ñ–ª—å–∫—ñ—Å—Ç—å –±–ª–æ–∫—ñ–≤ –∞–∫—Ç–∏–≤–∞—Ü—ñ—ó –≤ –∫–æ–∂–Ω–æ–º—É —à–∞—Ä—ñ
- –†–æ–∑–º—ñ—Ä –ø–∞–∫–µ—Ç—É

.center[$\vdots$]
]]
.kol-1-2[
**–ü–∞—Ä–∞–º–µ—Ç—Ä–∏**
.smaller-xx[
- –í–∞–≥–∏ —Ç–∞ –∑—Å—É–≤–∏ –ù–ú
- –¶–µ–Ω—Ç—Ä–æ—ó–¥–∏ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ —É –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
]]]

.footnote[Credits:: [Kizito Nyuytiymbiy](https://towardsdatascience.com/parameters-and-hyperparameters-aa609601a9ac)]

???
Hyperparameters are parameters whose values control the learning process and determine the values of model parameters that a learning algorithm ends up learning. The prefix ‚Äòhyper_‚Äô suggests that they are ‚Äòtop-level‚Äô parameters that control the learning process and the model parameters that result from it.

Parameters on the other hand are internal to the model. That is, they are learned or estimated purely from the data during training as the algorithm used tries to learn the mapping between the input features and the labels or targets.

---

class: middle

# –ó–∞–¥–∞—á–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó

## –ú—ñ–Ω—ñ–º—ñ–∑–∞—Ü—ñ—è –µ–º–ø—ñ—Ä–∏—á–Ω–æ–≥–æ —Ä–∏–∑–∏–∫—É (–≤—Ç—Ä–∞—Ç)

$$W\_\*^\{\mathbf{d}} = \arg \min\_W \mathcal{J}(W) = \arg \min\_W \frac{1}{n} \sum\_{i=1}^n \mathcal{L}\left(y^{(i)}, f(\mathbf{x}^{(i)}, W)\right)$$

---

class: black-slide

.width-100[![](figures/lec5/lossSurface.png)]

.footnote[Slide source: [Deep Learning in Computer Vision ](https://www.cs.ryerson.ca/~kosta/CP8309-F2018/index.html)]

---

class: middle

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó

–ù–∞–≤—á–∞–Ω–Ω—è –º–∞—Å–∏–≤–Ω–æ—ó –≥–ª–∏–±–æ–∫–æ—ó –Ω–µ–π—Ä–æ–Ω–Ω–æ—ó –º–µ—Ä–µ–∂—ñ —î —Ç—Ä–∏–≤–∞–ª–∏–º —Ç–∞ —Å–∫–ª–∞–¥–Ω–∏–º –ø—Ä–æ—Ü–µ—Å–æ–º.

–ü–µ—Ä—à–∏–º –∫—Ä–æ–∫–æ–º –¥–æ —Ä–æ–∑—É–º—ñ–Ω–Ω—è, –Ω–∞–ª–∞–≥–æ–¥–∂–µ–Ω–Ω—è —Ç–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –Ω–µ–π—Ä–æ–Ω–Ω–∏—Ö –º–µ—Ä–µ–∂ —î –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —ñ–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ñ–≤ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó:

- –ø–æ–±—É–¥–æ–≤–∞ –≥—Ä–∞—Ñ—ñ–∫—ñ–≤ –≤—Ç—Ä–∞—Ç —Ç–∞ —ñ–Ω—à–∏—Ö –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ,
- –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω–∏—Ö –≥—Ä–∞—Ñ—ñ–∫—ñ–≤,
- –ø–æ–∫–∞–∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö –ø—ñ–¥ —á–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è –º–µ—Ä–µ–∂—ñ.

---

class: blue-slide, middle, center
count: false

.larger-xx[–ü–∞–∫–µ—Ç–Ω–∏–π –≥—Ä–∞–¥i—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ 

GD]

---

class: middle

# –ü–∞–∫–µ—Ç–Ω–∏–π –≥—Ä–∞–¥i—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫

–©–æ–± –º—ñ–Ω—ñ–º—ñ–∑—É–≤–∞—Ç–∏ $\mathcal{J}(W)$, 
**–ø–∞–∫–µ—Ç–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫** (GD) –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –Ω–∞—Å—Ç—É–ø–Ω–µ –ø—Ä–∞–≤–∏–ª–æ:
$$\begin{aligned}
g\_t &= \frac{1}{n} \sum\_{i=1}^n \nabla\_W \mathcal{L}\left(y^{(i)}, f(\mathbf{x}^{(i)}, W)\right) = \nabla\_W \mathcal{J}(W)\\\\
W\_{t+1} &= W\_t - \alpha g\_t,
\end{aligned}$$
–¥–µ–∫ $\alpha$ &mdash; –∫—Ä–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è.
.center.width-60[![](figures/lec2/gd-good-2.png)]

---

class: middle

# GD

.larger-x[–ù–∞–π–≥—ñ—Ä—à–∏–π –º–µ—Ç–æ–¥ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó –≤ —Å–≤—ñ—Ç—ñ]

.alert[–ü—Ä–∏–º—ñ—Ç–∫–∞: –π–º–æ–≤—ñ—Ä–Ω–æ, –í–∞–º –Ω—ñ–∫–æ–ª–∏ –Ω–µ —Å–ª—ñ–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏ –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ –±–µ–∑–ø–æ—Å–µ—Ä–µ–¥–Ω—å–æ, –≤–≤–∞–∂–∞–π—Ç–µ —Ü–µ –±—É–¥—ñ–≤–µ–ª—å–Ω–∏–º –±–ª–æ–∫–æ–º –¥–ª—è —ñ–Ω—à–∏—Ö –º–µ—Ç–æ–¥—ñ–≤.]

.footnote[Credits: Aaron Defazio, Facebook AI Research]

---

class: middle

# –ö—Ä–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è

.center.width-70[![](figures/lec5/lr.png)]

–¢—É—Ç $\gamma = \alpha$ &mdash; –∫—Ä–æ–∫ –Ω–∞–≤—á–∞–Ω–Ω—è.

.footnote[Credits: Aaron Defazio, Facebook AI Research]

???
The learning rate $\gamma = \alpha$ can be set by the algorithm designer. If we use a learning rate that is too small, it will cause $W$ to update very slowly, requiring more iterations to get a better solution.

Typically we don't have a good estimate of the learning rate. Standard practice is to try a bunch of values on a log scale and use the one that gave the best final result.

Learning rates that are too large cause divergence where the function value (loss) explodes.

The optimal learning rate can change during optimization! Often decreasing it over time is necessary.

---


class: black-slide

.width-100[![](figures/lec5/surface.png)]

.footnote[Slide source: [Deep Learning in Computer Vision ](https://www.cs.ryerson.ca/~kosta/CP8309-F2018/index.html)]

???
The empirical risk is an average loss on the training dataset while the risk is the expected loss on the entire population of data.

There are many challenges in deep learning optimization. Some of the most vexing ones are local minima, saddle points, and vanishing gradients.

---

class: black-slide

.width-100[![](figures/lec5/surface2.png)]

.footnote[Slide source: [Deep Learning in Computer Vision ](https://www.cs.ryerson.ca/~kosta/CP8309-F2018/index.html)]

???
For any objective function $\mathcal{J}(W)$, if the value of $\mathcal{J}(W)$ at $W$ is smaller than the values of $W$ at any other points in the vicinity of $W$, then $\mathcal{J}(W)$ could be a local minimum. If the value of $\mathcal{J}(W)$ at $W$ is the minimum of the objective function over the entire domain, then $\mathcal{J}(W)$ is the global minimum.

---

class: black-slide, middle

.center[
<video loop controls preload="auto" height="600" width="600">
  <source src="./figures/lec5/follow-slope.mp4" type="video/mp4">
</video>
]


.footnote[Slide source: [Gradient descent, how neural networks learn](https://www.3blue1brown.com/lessons/gradient-descent)]

---

class: black-slide, middle

.center[
<video loop controls preload="auto" height="600" width="600">
  <source src="./figures/lec5/balls-rolling-down.mp4" type="video/mp4">
</video>
]


.footnote[Slide source: [Gradient descent, how neural networks learn](https://www.3blue1brown.com/lessons/gradient-descent)]

---



class: middle

.center[
<video loop controls preload="auto" height="600" width="600">
  <source src="./figures/lec5/opt-gd.mp4" type="video/mp4">
</video>
]

---

class: blue-slide, middle, center
count: false

.larger-xx[C—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∏–π –≥—Ä–∞–¥i—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫ 

SGD]

---


class: middle

# SGD

–©–æ–± –∑–º–µ–Ω—à–∏—Ç–∏ –æ–±—á–∏—Å–ª—é–≤–∞–ª—å–Ω—É —Å–∫–ª–∞–¥–Ω—ñ—Å—Ç—å, **—Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç–Ω–∏–π —Å–ø—É—Å–∫** (SGD) –ø–æ–ª—è–≥–∞—î –≤ –æ–Ω–æ–≤–ª–µ–Ω–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ–≥–æ –ø—Ä–∏–∫–ª–∞–¥—É
$$\begin{aligned}
\ell^{(i)} &= \mathcal{L}\left(y^{(i)}, f(\mathbf{x}^{(i)}, W)\right) \\\\
g^{(i)}\_t &= \nabla\_W \ell^{(i)} \\\\
W\_{t+1} &= W\_t - \alpha g^{(i)}\_t
\end{aligned}$$


???
In deep learning, the objective function is usually the average of the loss functions for each example in the training dataset. Given a training dataset of $n$ examples, we assume that $\ell^{(i)}$ is the loss function with respect to the training example of index $i$, where $W$ is the parameter vector.

---

class: middle

# –ü–µ—Ä–µ–≤–∞–≥–∏ SGD

- –°—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω—ñ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∏ –æ–±—á–∏—Å–ª—é–≤–∞—Ç–∏ –∑–Ω–∞—á–Ω–æ –ª–µ–≥—à–µ (–ø—Ä–æ–ø–æ—Ä—Ü—ñ–π–Ω–æ —Ä–æ–∑–º—ñ—Ä—É –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö), —Ç–æ–º—É –í–∏ —á–∞—Å—Ç–æ –º–æ–∂–µ—Ç–µ –∑—Ä–æ–±–∏—Ç–∏ —Ç–∏—Å—è—á—ñ –∫—Ä–æ–∫—ñ–≤ SGD –∑–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å –æ–¥–Ω–æ–≥–æ –∫—Ä–æ–∫—É GD.

- –£ —Å–µ—Ä–µ–¥–Ω—å–æ–º—É —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∏–π –≥—Ä–∞–¥—ñ—î–Ω—Ç —î —Ö–æ—Ä–æ—à–æ—é –æ—Ü—ñ–Ω–∫–æ—é –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞.

- –®—É–º –º–æ–∂–µ –ø–µ—Ä–µ—à–∫–æ–¥–∂–∞—Ç–∏ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ–π–Ω–æ–º—É —Å—Ö–æ–¥–∂–µ–Ω–Ω—é –¥–æ –ø–æ–≥–∞–Ω–∏—Ö –ª–æ–∫–∞–ª—å–Ω–∏—Ö –º—ñ–Ω—ñ–º—É–º—ñ–≤.
.footnote[Credits: Aaron Defazio, Facebook AI Research]

---

class: blue-slide, middle, center
count: false

.larger-xx[–ú—ñ–Ω—ñ-–ø–∞–∫–µ—Ç–∏]

---

class: middle

# –ú—ñ–Ω—ñ-–ø–∞–∫–µ—Ç–∏

–û–±—á–∏—Å–ª–µ–Ω–Ω—è –≤—Ç—Ä–∞—Ç –¥–ª—è –º—ñ–Ω—ñ-–ø–∞–∫–µ—Ç—ñ–≤ —Ç–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤
$$
\begin{aligned}
g^{(k)}\_t &= \frac{1}{B} \sum\_{i=1}^B \nabla\_W \mathcal{L}\left(y\_k^{(i)}, f(\mathbf{x}\_k^{(i)}, W)\right) \\\\
W\_{t+1} &= W\_t - \alpha g^{(k)}\_t,
\end{aligned}
$$
–¥–µ $k$ &mdash; —ñ–Ω–¥–µ–∫—Å –º—ñ–Ω—ñ-–ø–∞–∫–µ—Ç–∞.

- –ó–±—ñ–ª—å—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É –ø–∞–∫–µ—Ç—É $B$ –∑–º–µ–Ω—à—É—î –¥–∏—Å–ø–µ—Ä—Å—ñ—é –æ—Ü—ñ–Ω–æ–∫ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞ —Ç–∞ –∑–∞–±–µ–∑–ø–µ—á—É—î –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è –ø–∞–∫–µ—Ç–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏.
- –í–∑–∞—î–º–æ–∑–≤'—è–∑–æ–∫ –º—ñ–∂ $B$ —ñ $\alpha$ –≤—Å–µ —â–µ –Ω–µ–∑—Ä–æ–∑—É–º—ñ–ª–∏–π.

---


class: blue-slide, middle, center
count: false

.larger-xx[–Ü–º–ø—É–ª—å—Å]

---

class: middle

# –Ü–º–ø—É–ª—å—Å

SGD + –Ü–º–ø—É–ª—å—Å = –°—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–∏–π –º–µ—Ç–æ–¥ *–≤–∞–∂–∫–æ—ó –∫—É–ª—ñ*

$$
\begin{aligned}
p\_{t + 1} &= \beta\_t p\_{t} + \nabla \ell^{(i)} (W\_t)\\\\
W\_{t+1} &= W\_t - \alpha\_t p\_{t + 1}
\end{aligned}
$$

–ü—Ä–∞–≤–∏–ª–æ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è:

$$
\begin{aligned}
W\_{t+1} &= W\_t - \alpha\_t \nabla \ell^{(i)} (W\_t) + \beta\_t \left(W\_{t} - W\_{t-1}\right)
\end{aligned}
$$

**–ö–ª—é—á–æ–≤–∞ —ñ–¥–µ—è:** –ù–∞—Å—Ç—É–ø–Ω–∏–π –∫—Ä–æ–∫ —Å—Ç–∞—î –∫–æ–º–±—ñ–Ω–∞—Ü—ñ—î—é –Ω–∞–ø—Ä—è–º–∫—É –ø–æ–ø–µ—Ä–µ–¥–Ω—å–æ–≥–æ –∫—Ä–æ–∫—É —Ç–∞ –Ω–æ–≤–æ–≥–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç–∞.

???
Momentum replaces gradients with a leaky average over past gradients. This accelerates convergence significantly. Momentum usually speeds up the learning with a very minor implementation change. 

---


<iframe class="iframemomentum" src="https://distill.pub/2017/momentum/" scrolling="no" frameborder="no"  style="position:absolute; top:-165px; left: -25px; width:950px; height: 600px"></iframe>

.footnote[Credits:  Distill, [Why Momentum Really Works?](https://distill.pub/2017/momentum/)]

???
**Intuition.** The optimization process resembles a **heavy ball** rolling down a hill. The ball has **momentum**, so it doesn‚Äôt change direction immediately when it encounters changes to the landscape!

---

class: middle, black-slide

.center[
<video loop controls preload="auto" height="500" width="600">
  <source src="./figures/lec5/sgd-momentum.mp4" type="video/mp4">
</video>
]

.footnote[Image credits: Kosta Derpanis, [Deep Learning in Computer Vision](https://www.cs.ryerson.ca/~kosta/CP8309-F2018/index.html), 2018]

---

class: middle

# –ü–µ—Ä–µ–≤–∞–≥–∏

SGD –∑ —ñ–º–ø—É–ª—å—Å–æ–º –º–∞—î **—Ç—Ä–∏** —Ö–æ—Ä–æ—à—ñ –≤–ª–∞—Å—Ç–∏–≤–æ—Å—Ç—ñ:
- –≤—ñ–Ω –º–æ–∂–µ –ø—Ä–æ–π—Ç–∏ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—ñ –±–∞—Ä'—î—Ä–∏
- –ø—Ä–∏—Å–∫–æ—Ä—é—î—Ç—å—Å—è, —è–∫—â–æ –≥—Ä–∞–¥—ñ—î–Ω—Ç –Ω–µ —Å–∏–ª—å–Ω–æ –∑–º—ñ–Ω—é—î—Ç—å—Å—è
- –≥–∞—Å–∏—Ç—å –∫–æ–ª–∏–≤–∞–Ω–Ω—è —É –≤—É–∑—å–∫–∏—Ö –¥–æ–ª–∏–Ω–∞—Ö

---

class: middle

## –ü—Ä–∞–∫—Ç–∏—á–Ω—ñ –∞—Å–ø–µ–∫—Ç–∏ —ñ–º–ø—É–ª—å—Å—É

–ü–æ —Å—É—Ç—ñ, —Ü–µ **¬´–±–µ–∑–∫–æ—à—Ç–æ–≤–Ω–∏–π –æ–±—ñ–¥¬ª**, –º–∞–π–∂–µ –≤ —É—Å—ñ—Ö —Å–∏—Ç—É–∞—Ü—ñ—è—Ö **SGD + —ñ–º–ø—É–ª—å—Å** –∫—Ä–∞—â–µ, –Ω—ñ–∂ SGD, —ñ –¥—É–∂–µ —Ä—ñ–¥–∫–æ –≥—ñ—Ä—à–µ!

## –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏:

$\beta = 0.9$ or $0.99$ –º–∞–π–∂–µ –∑–∞–≤–∂–¥–∏ –ø—Ä–∞—Ü—é—Ç—å –¥–æ–±—Ä–µ. –Ü–Ω–æ–¥—ñ –º–æ–∂–Ω–∞ –æ—Ç—Ä–∏–º–∞—Ç–∏ –Ω–µ–≤–µ–ª–∏–∫—ñ –ø–µ—Ä–µ–≤–∞–≥–∏, –Ω–∞–ª–∞—à—Ç—É–≤–∞–≤—à–∏ –π–æ–≥–æ.

–ü–∞—Ä–∞–º–µ—Ç—Ä —Ä–æ–∑–º—ñ—Ä—É –∫—Ä–æ–∫—É ($\alpha$) –∑–∞–∑–≤–∏—á–∞–π –ø–æ—Ç—Ä—ñ–±–Ω–æ –∑–º–µ–Ω—à—É–≤–∞—Ç–∏, –∫–æ–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä —ñ–º–ø—É–ª—å—Å—É –∑–±—ñ–ª—å—à—É—î—Ç—å—Å—è, —â–æ–± –ø—ñ–¥—Ç—Ä–∏–º—É–≤–∞—Ç–∏ –∑–±—ñ–∂–Ω—ñ—Å—Ç—å.

.footnote[Credits: Aaron Defazio, Facebook AI Research]

---

class: end-slide, center
count: false

.larger-xxxx[üèÅ]


